#Nescience library:
from Nescience.Nescience import Nescience, Miscoding, Inaccuracy, Surfeit

#Dataset loading:
from datasets import launch_data

#Genetic Algorithm Library (PonyGE2)
from fitness.base_ff_classes.base_ff import base_ff

#Math, Data Science and System libraries
import numpy  as np
import pandas as pd
import math, collections
import time , sys, os, pickle
from random import randint
import matplotlib.pyplot as plt

# Compression algorithms
import bz2, lzma, zlib

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Keras & tensorflow
import keras
from keras.layers import Dense 
from keras.models import Model, Sequential
from keras import optimizers, losses
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras import backend as K
import tensorflow as tf

#Debugging and logging
import logging, csv, traceback, pdb, code
from datetime import datetime
from multiprocessing import Value
from tqdm import tqdm 

#Shared variable between multiprocessing Pool (accessing ind. ID)
counter = Value('i',0) 

#Disabling some (annoying) TF Info logs
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.get_logger().setLevel(logging.ERROR)
tf.logging.set_verbosity(tf.logging.ERROR)


class NescNNclasGE(base_ff):
    global counter #Contains individual ID number
    counter = Value('i',0)
    maximise = False #we want to minimize our objective: Nescience value.
    
    def __init__(self):
        
        super().__init__()
        #class (fixed) attributes
        self.it         = 100
        self.lr         = 0.01
        self.verbose    = True
        self.optimizer  = None
    
        #Generating Nescience stats csv file for algorithm post-run analysis
        self.nsc_csv_name = datetime.now().strftime('%H:%M:%S.csv')

        #Data load and preprocessing
        scaler = StandardScaler()

        #Available data: load_pulsars, load_magic, load_fashionmnist, load_digitsdata, load_irisdata
        # "reserved" data is not used in this algorithm but saved for use in the final 
        # model evaluation outside the script. Important keeping same random_state and test_size.
        #Note: FashionMNIST does NOT need this initial traintestsplit (already done)
        X, y , dataset = launch_data()

        if "fmnist" not in dataset:
            X, X_reserved, y, y_reserved  = train_test_split(X,y, random_state = 42,test_size = 0.33)

        self.classes_, self.y = np.unique(y, return_inverse=True)
        self.n_classes = self.classes_.shape[0]

        #Test split is used for evaluating individual's Nescience.
        #self.X, self.X_test, self.y, self.y_test  = train_test_split(X,self.y, test_size = 0.2)
        #self.X = scaler.fit_transform(self.X)
        #self.X_test = scaler.transform(self.X_test)
        
        self.X = scaler.fit_transform(X)
        print("Input train data shape: {}. Output: {}. Number of classes: {}.".format(self.X.shape, self.y.shape, self.n_classes))

        #Storing scaling factors (mean and var) in file for later use
        self.save_scaling(scaler)  

        self.X = np.array(self.X)
        #self.X_test = np.array(self.X_test)
        self.n_vars = self.X.shape[1]

        self.nescience = Nescience()
        self.nescience.fit(self.X, self.y) #Miscoding, Surfeit and Inacc. initialized here

        miscoding, self.vius = self.compute_miscoding_list()
        num_vius = [len(np.where(viu)[0]) for viu in self.vius]
        self.misc_vius_list = list(zip(num_vius,self.vius, miscoding))
        
        print("Done!")

        vius_filename = "./analysis/"+self.nsc_csv_name[:8]+"/"+"vius.txt"
        os.makedirs(os.path.dirname(vius_filename), exist_ok=True)
        with open(vius_filename, 'wb') as f:
            pickle.dump(self.vius, f)   

        self.nn = None #Model (that will be tested, etc)

        return


    def evaluate(self, ind, **kwargs):
        
        '''
        Fitness function that will evaluate each individual generated by the
        genetic algorithm. An individual will be generated from the grammar and
        will be executed as Python code. 

        After the resulting network is evaluated, will produce a nescience score and
        that will be used for evaluating the fitness of the individual.

        The best individual will be that belonging to the generation which achieved
        lowest Nescience.
        '''        
        global counter 
        inargs = {"self": self}

        try:
            exec(ind.phenotype, inargs)
            #Obtain generated output from exec (grammar execution) dictionary
            self.viu, self.nn, self.msdX = inargs['viu'], inargs['nn'], inargs['msdX']
            self.optimizer = self._create_optimizer(inargs['opt'])

            self.nn.compile(loss=losses.sparse_categorical_crossentropy,
                            optimizer=self.optimizer, 
                            metrics=['accuracy'])

            #Keras model checkpoint file creation
            fname = "./analysis/"+self.nsc_csv_name[:8]+"/networks/"+"Net"+str(counter.value)+'_fullmodel.hdf5'
            os.makedirs(os.path.dirname(fname), exist_ok=True)

            early_stop = EarlyStopping(monitor='val_acc',mode='max', verbose=0,patience = 10)
            cp_save = ModelCheckpoint(fname, save_best_only=True,verbose=0, monitor='val_acc', mode='max')            
    
            self.nn.fit(x=self.msdX, y=self.y, 
                        validation_split = 0.33,
                        verbose=0, 
                        batch_size=32, 
                        epochs=self.it, 
                        callbacks = [early_stop, cp_save])

            #Restores network from best epoch
            self.nn.load_weights(fname)

            #msdX_test = self.X_test[:, np.where(self.viu)[0]]
            #nsc = self._nescience(self.viu, self.nn, msdX_test)

            msdX = self.X[:, np.where(self.viu)[0]]
            predictions = self.predict(self.nn, msdX)

            nsc, surfeit, inaccuracy, miscoding = self.nescience.nescience(model = self.nn,
                                                                           subset = list(self.viu), 
                                                                           predictions = list(predictions))
            
            vals = self._update_vals(surfeit, inaccuracy, miscoding, nsc)
            num_vius = np.count_nonzero(self.viu == 1)

            #Data storing (nescience parameters) for later analysis
            nsc_data = [counter.value,num_vius,vals["miscoding"], vals["inaccuracy"],
                        vals["surfeit"], vals["nescience"]]
            self.save_ind(nsc_data) 

            print("IND#",counter.value,"[", num_vius, "VIUs] Num.param:",self.nn.count_params(),"Inaccuracy:", vals["inaccuracy"], "Score:", vals["score"],
                  "Miscoding:", vals["miscoding"], "Surfeit:", vals["surfeit"], "Nescience:", vals["nescience"])
            
            counter.value += 1
            K.clear_session()
        except:
            type, value, tb = sys.exc_info()
            traceback.print_exc()
            last_frame = lambda tb=tb: last_frame(tb.tb_next) if tb.tb_next else tb
            frame = last_frame().tb_frame
            ns = dict(frame.f_globals)
            ns.update(frame.f_locals)
            code.interact(local=ns)
            nsc = 0.99 #invalid network has high nsc (very bad)

        return nsc #this will be the target to minimize by the GE algorithm.

    def compute_miscoding_list(self):
        '''
        Computes miscoding list and the corresponding list of vius.
        '''
        viu = np.zeros(self.n_vars,dtype=np.int)
        vius = []
        msd = self.nescience.miscoding.miscoding_features() 
        msd_list = []

        #Generates list of VIUs in ascendo based on best-criteria
        for _ in tqdm(np.arange(self.n_vars)):
            msd[np.where(viu)] = 0
            viu[np.where(msd == np.max(msd))] = 1
            vius.append(viu.copy())
            subset_mscd = self.nescience.miscoding.miscoding_subset(viu)
            msd_list.append(subset_mscd)

        #Array containing miscoding from 1 to N attributes in use
        miscoding = np.array(msd_list)

        return miscoding, vius

    def save_scaling(self, scaler):
        """
        Stores the value of the parameters used by the scaler and saves it for
        later retrieval on model evaluation. 
        """

        (s_mean, s_var) = scaler.mean_, scaler.var_

        scaling_filename = "./analysis/"+self.nsc_csv_name[:8]+"/"+"scaling_factors.txt"
        os.makedirs(os.path.dirname(scaling_filename), exist_ok=True)
        with open(scaling_filename, 'wb') as f:
            pickle.dump((s_mean, s_var), f) 

        return


    def _create_optimizer(self, opt):
        if "sgd" in opt:
            return optimizers.SGD()
        if "adam" in opt:
            return optimizers.adam()
        if "Adadelta" in opt:
            return optimizers.Adadelta()

    def save_ind(self, nsc_data):
        '''
        Saves the current individual (nescience stats) being evaluated by the algorithm in a json file and 
        appends a new row to the Nescience stats csv file created for the alrorithm run.
        
        The network architecture can also be stored so that it can be later loaded with the keras
        function "model.from_json()" function. Note that the architecture DOES NOT save the weights, 
        only the model structure, and it is needed to recompile and fit the network after loading it with the method.
        '''

        stat_file = "./analysis/"+self.nsc_csv_name[:8]+"/"+"stats.csv"
        os.makedirs(os.path.dirname(stat_file), exist_ok=True)
        with open(stat_file, 'a') as csvFile:
            writer = csv.writer(csvFile)
            writer.writerow(nsc_data)

        return


    def _update_vals(self, surfeit, inaccuracy, miscoding, nescience):

        vals = dict()

        vals["surfeit"]     = surfeit
        vals["inaccuracy"]  = inaccuracy
        vals["miscoding"]   = miscoding
        vals["nescience"]   = nescience
        vals["score"]       = self._score(self.nn, self.viu)

        return vals

    def predict(self, nn, X):
        """
        Predict class given a dataset
          * X = list([[x11, x12, x13, ...], ..., [xn1, xn2, ..., xnm]])
    
        Return a list of classes predicted
        """
        # TODO: Check that we have a model trained
        predictions = nn.predict(X)
        predictions = np.argmax(predictions, axis=1)
        return predictions

    def _score(self, nn, viu):
        """
        Computes and print Keras score for the test data.
        """
        X = self.X[:,np.where(viu)[0]]
        #score = nn.evaluate(x, self.y_test, verbose=0)
        score = nn.evaluate(X, self.y, verbose = 0)

        return score[1]

